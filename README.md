# **Machine Learning for Digital Fraud Detection**
### Comparing the Performance of Stacking Classifier & Deep Learning Models in Identifying Instances of Digital Transaction Fraud



Brian Morrison

DATA606 - Capstone Project in Data Science

Professor Jay Wang

The University of Maryland, Baltimore County

## **Contents**
* [Introduction & Background](#introduction--background)

* [Dataset Overview](#dataset-overview)

* [Source Data Splitting](#source-data-splitting)

* [EDA & Dataset Preparation](#eda--dataset-preparation)

* [Ensemble Classification Model](#ensemble-classification-model)

* [Deep Learning Model](#deep-learning-model)

* [Conclusions & Final Thoughts](#conclusions--final-thoughts)

* [References](#project-references)

## **Introduction & Background**

The advent of cashless methods for payment transactions, such as credit cards, digital wallets, and buy now pay later (BNPY) services has incomparably augmented the way in which consumers interact with product and service providers. In fact, market research has indicated that as many as 80% of consumers used some form of digital payment - defined as including "...browser-based and in-app online purchases, in-store checkout using a mobile phone and/or QR code, and person-to-person payments..." in 2020, with nearly 60% of those consumers reporting using two or more forms of digital payment<sup>1</sup>. Cash and check transactions have been becoming increasingly rare relative to more convenient and secure payment methods, such as credit and debit cards - which accounted for over 50% of payment usage in 2019<sup>2</sup> and are almost ubiquitously linked to online transactional systems. While this shift has introduced a number of positive benefits to consumers, another area it has significantly affected is fraud; instances of attempted digital payment fraud have increased dramatically in the past few years, both in quantity and magnitude<sup>3,4</sup>. Given the compounding growth of digital transaction methods, and the resulting surge in fraudulent digital payment attempts, developing robust methods for identifying and classifying instances of fraud is a similarly compounding topic of interest to digital system providers.

Problem Statement: There are more ways to commit payment fraud today than there ever have been, and this statement will again be true in 10 years. Building and deploying a plethora of machine-learning based fraud detection systems, each with different approaches to classifying potential instances of fraud, is likely the best way to combat the wide range of ways individuals can commit fraud. In this project, I propose methods for constructing robust fraud detection models, with a specific emphasis on comparing the performance of Ensemble and Deep Learning based classification models.

## **Dataset Overview**

The dataset for this project is a large log of e-commerce transactions with a range of associated details. The dataset can be accessed [here](https://www.kaggle.com/competitions/ieee-fraud-detection/data). The data was made available through a Kaggle competition held by the IEEE Computational Intelligence Society (IEEE-CIS), and contains real-world ecommerce transactions provided by Vesta, an organization actively targeting digital fraud. 

The dataset is 1.35 GB in size, comprised of four csv files - two training and two testing files, separated by the type of features they contain. Two of the files contain transactional data, basic information about the transaction such as product and payment details, and two of the files contain identity data, information about the purchaser such as device and personal information. The transactional data contains 392 unique numerical and categorical features, while the identity data contains 40 unique numerical and categorical features. The transactions can be correlated between datasets by an identifying feature, 'TransactionID'. 

For this project, only the training dataset will be used, as it contains the target variable that models will be attempting to predict - 'isFraud', a binary variable indicating whether or not the transaction was fraudulent. The testing dataset's class labels were withheld by competition hosts, as test data predictions generated by models were submitted for consideration in project scoring. The training dataset contains over 590,000 individual transactions, with the proportion of fraudulent to legitimate transactions standing at nearly 1 to 30; roughly 3.5% of the dataset.

## **Source Data Splitting**

The first step in beginning this project is to migrate the dataset from its source location, Kaggle, to the project's GitHub repository. While GitHub provides a simple interface to manually upload data, there is a maximum file size limit of 50 MB. The maximum file size limit for data uploaded to a repository through the command line, however, is 100 MB - a much more reasonable limit considering our transactional dataset's size of over 600 MB.

In this notebook, we take steps to split the transactional dataset into 7 files below 100 MB to upload to the project's repository. By using pandas to import the data into a dataframe, we can easily split the dataset into multiple smaller dataframes ready for export. Importantly, these dataframe were split row-wise to support later concatenation through pandas .concat method. The identification dataset was itself below 100 MB, so it did not need to be split before being uploaded to the repository. 

Finally, a brief overview of the Git commands leveraged in uploading the data files to the project repository is provided for context. While many of these commands can be executed through a masked interface in the Visual Studio Code IDE, consideration of the Git commands popularly used for version control allows for some contextualization of the approach to data migration. An overview of the Git commands discussed is included below.

* `$git --version` - *Checking Git version*
* `$git clone https://github.com/briancmorrison/brian_data606.git` - *Cloning the project repository to make local edits*
* `$cd brian_data606` - *Navigating to cloned directory*
* `$git add ['filenames']` - *Adding files to local repository*
* `$git commit -m "Uploaded Source Data"` - *Commiting changes locally, with '-m' specifying the commit message*
* `$git push origin master` - *Pushing changes to GitHub repository, reconciling versions*

After confirming that the dataset additions were made to the GitHub repository, we are ready to move on to dataset exploration, cleaning, and preparation.

## **EDA & Dataset Preparation**

After handling necessary preparation steps for the source data files, the next step in the project is to explore, clean, and prepare data for introduction to machine learning models. Throughout the EDA & Dataset Preparation notebook, changes are made to align data parameters with those expected by many machine learning models - more specifically, changes are largely oriented towards gradient or distance-based models that may be sensitive to differences in the magnitude of variances in features. For example, a distance-based classifier may interpret differences in transaction amounts as more meaningful than differences in number of transactions made by cards due to large discrepancies in purchase amounts, despite the assumption in training that all features should be considered equally.

Thus, the desired outcome from this notebook is a dataset that contains only integer columns with no null values, and whose column ranges are scaled to be approximately (0, 1). To start, the split datasets were read into the notebook from the project's GitHub repository, and stored in a dictionary of pandas dataframes. The transactional dataframes were then concatenated using pandas .concat method. A left outer merge, using pandas .merge method, was then performed to combine the transaction and identity dataframes, with the column 'TransactionID' used as the merge key. A left outer merge was used to ensure all transactions were retained in the final dataset.

### Null Handling & Feature Selection

After some basic data exploration, the first, and arguably most significant, decision to be made in this notebook is how to handle the high proportion of null values present in the dataset. Figure 1 below shows the proportion of null values in each column of the source dataset. The figure clearly displays an extremely high quantity of null values across a majority of features in the dataset, with only a small number of columns containing no null values. 

**Figure 1** - Proportion of null values in each of the combined dataset's 434 columns.

![image](https://user-images.githubusercontent.com/80338181/183307425-a5f42ec1-d7c7-434e-84fa-e007cd2faead.png#gh-light-mode-only)
![image](https://user-images.githubusercontent.com/80338181/183307470-2335ae61-f8c8-4b69-8568-0d1a4da2753d.png#gh-dark-mode-only)

There are a multitude of approaches to handling null values in a dataset, including value imputation, feature combination, dimensionality reduction, or simple removal. While each method generally has unique advantages and potential disadvantages, this project excluded any columns containing null values based on Google Colaboratory resource constraints. This is discussed in more detail in the future directions portion of the Conclusions & Final Thoughts section, dropping data points that may contain salient information for models to become attuned to is almost certain to degrade overall classification performance. 

This approach resulted in 20 remaining columns, with 1 being the Transaction ID identifying key and 1 being the isFraud class identifier - so a total of 18 features were retained for the models. They included the transaction date, transaction amount, product type, card type, and several counting features. One additional feature, card type, was also removed based on resource constraints, as encoding the categorical feature would have resulted in the addition of over 700 new columns. After this, 17 features remained for the models to be trained on.

### Feature Encoding & Scaling

Following the removal of null values, the next step in preparing our dataset for introduction to machine learning models is encoding categorical variables. A significant number of categorical features contained null values and were therefore removed in the previous step. However, the remaining categorical column, product type, needed to be converted to a numerically typed column through One Hot Encoding - a process by which unique categories from the feature are "popped" out into separate columns, with binary values indicating whether or not the record is associated with the category. The process is relatively straightforward, and resulted in the addition of 5 new columns to the dataset.

After encoding our categorical feature, it was possible to move on to scaling data - a requirement for many distance and gradient based machine learning models. Feature scaling involves standardizing features to a range of approximately (0, 1) and a mean value of approximately 0. This allows equal consideration of features by distance or gradient based models, which might otherwise assume that a larger magnitude of variance in one feature relative to others indicates heightened importance of that feature. Scaling is also a prerequisite for Principal Components Analysis, a dimensionality reduction method that is discussed in more detail in the Minority Class Augmentation section. Boxplots will be used to visualize the range and approximate percentile distributions of our features to guide scaling, with Figure 2 belowing showing the distribution of features, excluding transaction datetime, prior to scaling.

**Figure 2** - Feature distributions prior to scaling.

![image](https://user-images.githubusercontent.com/80338181/183308390-fad2419a-08a9-4292-b6d3-ed37beb908c2.png#gh-light-mode-only)
![image](https://user-images.githubusercontent.com/80338181/183308507-ec7cca50-2eaf-453f-94bd-ce68fa1bb897.png#gh-dark-mode-only)

In selecting a suitable scaler to use on data, considering distributions and percentiles, as well as the presence of outliers, can help guide decision-making. For our primary features visualized above, 3 types of scalers were tested to assess their effectiveness. They are:

* *Standard Scaler* - A relatively simple scaling method where the mean of the distribution is subtracted from each value, and then values are scaled to unit variance - or divided by the standard deviation of the distribution.
* *Robust Scaler* - A more sophisticated scaling method that is intended to provide enhanced tolerance for extreme outliers in leptokurtic data. The median is removed from the distribution and data is scaled according to the Interquartile Range (IQR). 
* *MinMax Scaler* - A scaler designed for feature confined to a definite range with roughly uniform distribution. Values are scaled such that the minimum feature value is equal to 0, and the maximum feature value is equal to 1. 

The MinMax Scaler was shown to outperform the Standard Scaler across all columns, as was expected based on the non-Gaussian distributions of the features. A relatively unexpected outcome was the MinMax Scaler also outperforming the Robust Scaler across all columns, despite the high prevalence of outliers in some features. With some exceptions, outliers did not appear to significantly influence percentile ranges, which may explain why the simple MinMax Scaler performed so well relative to the other methods. Figure 3 below shows the previously displayed feature distributions following scaling. 

**Figure 3** - Feature distributions following scaling. 

![image](https://user-images.githubusercontent.com/80338181/183308415-3b1d8cd2-13f4-40b6-beac-5a9384416bc9.png#gh-light-mode-only)
![image](https://user-images.githubusercontent.com/80338181/183308542-eb619dd6-7306-4392-be83-837be452ebd9.png#gh-dark-mode-only)

Following feature scaling, data was split into training and testing sets, with 30% of the data being set aside for later model evaluation. Importantly, while using scikit-learn's train_test_split function to divide data, the field 'isFraud' needed to be passed to the stratify parameter of the function to ensure equal numbers of legitimate and fraudulent transactions were included in the training and testing data. 

### Minority Class Augmentation

After scaling our features, the final step in preparing data to be passed to machine learning models is remedying the class imbalance present in the dataset. In many binary classification problems like transactional fraud detection, there are magnitudes fewer instances of one class than another. Considering the volume of transactions made through digital mediums every day, and the limited number of individuals attempting to commit fraud, it is no surprise that our dataset contains many more legitimate transactions than fraudulent ones. Figure 4 below displays the distribution of class labels across our dataset - instances labeled as fraud account for only 3.5% of the total volume of transactions.

**Figure 4** - The training data class imbalance visualized. 

![image](https://user-images.githubusercontent.com/80338181/183318868-3dcd96f7-94b4-47b1-8c49-18a37f11e0f7.png#gh-light-mode-only)
![image](https://user-images.githubusercontent.com/80338181/183318939-145ad114-26e1-4226-b9b4-3de3034945b4.png#gh-dark-mode-only)

In order to ensure machine learning models have a sufficient volume of data to be trained on with regards to the class of interest, it is important to artificially balance the quantities of data instances in each class. This intervention is called minority class augmentation, and can be done in a number of ways. Two common methods for augmenting minority classes are adjusting data sampling, by selectively oversampling the minority class or undersampling the majority class, and imputing novel data points, often through synthetic data generation techniques such as nearest neighbor algorithms. For this project, two approaches to incorporating additional data points into the minority class were selected, and are detailed below. 

* *SMOTE* - Synthetic Minority Over-Sampling Technique, or SMOTE, is a sophisticated approach to minority class augmentation that creates synthetic data points similar to existing data points. SMOTE employs a nearest neighbors approach to generating synthetic data points, by which neighbors close in feature space to existing minority class observations are generated. While a more advanced class balancing technique, this approach relies on the assumption that minority class observations are close in feature space to one another.

* *Random Oversampling* - Random Oversampling, or ROS, is a simpler approach to minority class augmentation that involves randomly sampling minority class observations to duplicate in the dataset, artificially balancing class weights across the dataset. This approach to handling class imbalances is particularly effective when minority class observations are not always close in feature space, meaning that synthetic data points generated through nearest neighbor algorithms may not accurately capture the salient features of minority class observations.

As is mentioned in the SMOTE overview, closeness in feature space of fraud instances - essentially, how similar they are across all of the features considered by the machine learning models - may impact the effectiveness of our class balancing techniques. An approach involving the creation of synthetic data points that are near neighbors to existing data points relies on the assumption that those instances are in fact near neighbors of one another. It is worth examining the distribution in a collapsed feature space of our data points to identify whether instances of fraud appear to show closeness to one another, using an approach called Principal Component Analysis (PCA).

#### PCA & Feature Space Evaluation

PCA is a method of dimensionality reduction often used to collapse a number of features into a smaller number of aggregate features that maintain similar directional trends. Briefly, PCA relies on computing the eigenvectors and eigenvalues of a covariance matrix generated by converting features into directional arrays, which are used to guide the construction of combined features that capture as much of the original variance in features as possible. Using PCA, we are able to collapse our feature space down to 3 aggregate features, and examine the distribution of fraud instances across these features. If fraud instances are close in fetaure space, it is likely an indication that SMOTE will be a more effective method for class balancing.

After fitting our PCA model to our data and ensuring that a significant amount of feature variance was captured by its componens (C1: 0.642, C2: 0.217, C3: 0.072), it is possible to visualize the results in three-dimensional space and assess the class distributions using hues. The outcome of this effort is shown in Figure 5 below, where the lighter blue points are instances of fraud.

**A Quick Note** - T-distributed stochastic neighbor embedding, or tSNE, is another popular approach to dimensionality reduction that is particulary well suited for visualizing high-dimensional data. 

**Figure 5** - Three-dimensional visualization of data distribution across principal components. 

![image](https://user-images.githubusercontent.com/80338181/183309155-d2167b57-a869-4c83-aeba-cec4697c0580.png#gh-light-mode-only)
![image](https://user-images.githubusercontent.com/80338181/183309202-f1f9f6b2-3115-4d0c-835d-9424e1366adb.png#gh-dark-mode-only)

It appears that the instances of fraud within the dataset are relatively close in feature space based on the principal components generated. For this reason, I would expect that SMOTE will perform well as a method for minority class augmentation. For the sake of comparison, we can generate separate training data for our SMOTE and ROS methods to compare resulting model performance, using a similar split and export approach as previously to make sure we can upload everything to the project's repository.

#### SMOTE

To generate and impute values using SMOTE, we will need to fit the SMOTE model to our training data, passing the parameters as a pandas dataframe of features and an array of class labels. The model defaults to using 5 nearest neighbors to generate synthetic points, which can be tuned if necessary. The code for performing the model fitting and synthetic data generation and imputation is included below.

```
from imblearn.over_sampling import SMOTE

columns = list(train_data.columns)

columns.remove('isFraud')

X_SMOTE, y_SMOTE = SMOTE().fit_resample(train_data[columns], train_data['isFraud'])
```

Follwing this, we can examine our new class distributions to confirm that the balancing was successful. Figure 6 below displays the class distributions following SMOTE application. 

**Figure 6** - Visualization of the class distributions across the SMOTE dataset. 

![image](https://user-images.githubusercontent.com/80338181/183318997-7f964aee-6dc2-4c7d-81a9-088c642c1332.png#gh-light-mode-only)
![image](https://user-images.githubusercontent.com/80338181/183319123-eef68ba5-912f-4d63-8973-7170d807af76.png#gh-dark-mode-only)

#### Random Oversampling

The process for generating and impute values using Random Oversampling is similarly straightforward, involving passing a pandas dataframe of features and an array of class labels to the model to fit and generate identical samples of the minority class. The code for performing the model fitting and data sampling is included below.

```
from imblearn.over_sampling import RandomOverSampler

y_train = train_data.pop('isFraud') 

oversampler = RandomOverSampler(random_state = 4)

X_ROS, y_ROS = oversampler.fit_resample(train_data, y_train)
```

As with the SMOTE example, can examine our new class distributions in Figure 7 below. After handling our class imbalance issue, we are ready to progress on to creating, training, and evaluating machine learning models. 

**Figure 7** - Visualization of the class distributions across the ROS dataset.

![image](https://user-images.githubusercontent.com/80338181/183319161-be4bcbe4-429c-49ce-97f4-0668e8372650.png#gh-light-mode-only)
![image](https://user-images.githubusercontent.com/80338181/183319203-a0b53f1c-afe2-4dda-9496-717172e0701c.png#gh-dark-mode-only)

## **Ensemble Classification Model**

We will start by creating and evaluating an Ensemble Classification Model. Ensemble Classifiers are machine learning models comprised of multiple smaller classifiers, in which the output of the preceding classifier is passsed as an input to the succeeding classifier, ending with a final classifier that outputs a class prediction. Generally, this type of classifier is used to combine multiple weak learning models to form a composite model with a higher degree of predictive validity. While not always the case, one advantage of Ensemble Classifiers is their ability to become attune to salient patterns in data in a more resource efficient manner than more 'greedy' models such as neural networks.

To guide selection of smaller classification models to comprise the Ensemble Model, LazyPredict - a Python library for evaluating simple classifier performance - was used. LazyPredict fits a number of classification models to a small subset of data, and returns an object displaying each fitted classifier sorted by its classification metrics such as accuracy and F1 score. 

### Lazy Predict

The output of fitting our LazyPredict model to the data was a multitude of models with perfect accuracy and F1 scores of 1.00. While this was likely a result of overfitting to a small sample of data and not necessarily reprentative of broader model strength, the output did show high performance among several resource efficient models. Three models from the group of perfect scorers - AdaBoost Classifier, Linear Discriminant Analysis or LDA, and XGB Classifer - were selected to comprise our final model. Additionally, a Logisitic Regression model was selected to be used as the final, authoritative estimator to output a final class label prediction. 

A brief description of the models comprising the Ensemble Classifier:

* *AdaBoost Classifier* - Adaptive Boosting, or AdaBoost, is a model that itself is an Ensemble Classifier, fitting multiple "weak learners" in succession and reconciling the incorrect predictions made by preceding models to improve those later in the chain. Generally, AdaBoost models are comprised of multiple shallow tree-based models which individually hold limited validity as classifiers.
* *Linear Discriminant Analysis* - Linear Discriminant Analysis, or LDA, is linear approach to classification that is also widely leveraged as an approach to dimensionality reduction, similar to PCA. It is effective at projecting features into lower dimensional spaces and identifying linear separations between classes where they exist.
* *XGBoost Classifier* - Extreme Gradient Boosting, or XGBoost, is a popular implementation of gradient-boosted decision trees that, similar to AdaBoost, is comprised of multiple weak learning classifiers. The model uses a gradient descent algorithm to minimize the loss function among later models, identifying areas of descent along the loss convex that might indicate an absolute minimum. 

### Model Creation

After selecting our components classifiers, we are able to build a Stacking Classifier using scikit-learn's StackingClassifier. A Stacking Classifier is a type of Ensemble Model in which component models are "stacked" on top of one another, with the preceding model's output being received as an input to the succeeding model. The code for instantiating our Stacking Classifier is included below.

```
from sklearn.ensemble import StackingClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
import xgboost as xgb
from sklearn.linear_model import LogisticRegression

abc = AdaBoostClassifier()
lda = LinearDiscriminantAnalysis()
xgb = xgb.XGBClassifier()

classifiers = [('adaboost', abc), ('lda', lda), ('xgboost', xgb)]

stack = StackingClassifier(classifiers, final_estimator = LogisticRegression())
```

Importantly, it should be noted that no hyperparameter tuning or cross-validation was performed when instantiating and aggregating these models. This decision was primarily made on the basis of Google Colaboratory resource constraints, and is discussed in more detail in the Future Directions section. The models comprising the Stacking Classifier are used with the default hyperparameters, which likely impacted resulting performance in novel classification tasks. The model as is is intended for use as a comparison relative to a simple Deep Learning classifier, and not as a final version. 

### Model Evaluation

After fitting the model to the training data, predictions were generated based on test data features and compared to the actual class labels provided for each data instance. The confusion matrices for both the model fit to the SMOTE data and the model fit to the ROS data are included in Figures 8 and 9 below. As accuracy is not a meaningful metric by which to assess binary classification models, especially in instances of significant class imbalances as is the case with this dataset, model evaluation will be based on the outcomes of the confusion matrices and the models' combined F1 scores - a figure representing the harmonic mean of precision and recall.

**Figure 8** - Ensemble Model classification matrix - SMOTE data.

![image](https://user-images.githubusercontent.com/80338181/183319363-8f89fae1-0803-493f-9498-860b9e67c54e.png#gh-light-mode-only)
![image](https://user-images.githubusercontent.com/80338181/183319514-67fd3652-186c-4f56-b736-bd344579515f.png#gh-dark-mode-only)

**Figure 9** - Ensemble Model classification matrix - ROS data.

![image](https://user-images.githubusercontent.com/80338181/183319470-ca4c339a-4d4f-4382-90bc-297ab0ab77ac.png#gh-light-mode-only)
![image](https://user-images.githubusercontent.com/80338181/183319555-11f13f7d-1019-4620-a6b7-95dcdebcea7d.png#gh-dark-mode-only)

As expected based on our feature space evaluation, the model trained on the SMOTE data slightly outperformed the model trained on the ROS data, eliciting a combined F1 score of 0.64 relative to 0.57. The SMOTE model appeared more precise in its classifications, mislabeling legitimate transactions as fraudulent at a significantly lower rate. However, the 'wider' net cast by the ROS model seemed to let less fraudulent transactions slip past it, resulting in a higher number of fraudulent transactions being labeled as such and a lower number of fraudulent transactions being classified as legitimate. The potential utility of these models will be discussed further in the Conslusions & Final Thoughts section. 

## **Deep Learning Model**

After evaluating our Stacking Classifier, we can progress on to building a Deep Learning model and evaluating its performance relative to the Ensemble Model. Deep Learning is a machine learning approach involving the use of neural networks, large webs of connected nodes with individually tuned activation functions that affect their output, to assess patterns in data features. While adept at discerning discrete interactions between data features, deep learning models require a substantial amount of data to be trained efficiently, and can often be less resource-efficient than simpler models such as those used in our Stacking Classifier.

One important consideration here is that, unlike some other classification or dimensionality reduction models used in this project like Logistic Regression or PCA, Deep Learning is not distance-based and therefore does not require the robust feature scaling covered in our EDA & Dataset Preparation notebook. While the same cleaned and scaled data that was used for the Stacking Classifier will be used for the Deep Learning model for continuity, it would be possible to use unscaled features for this model.

### Consideration - Transfer Learning

Prior to creating our Deep Learning model, it is important to consider the significant role that widely available, pre-trained models have on this space. Transfer Learning is a process by which pre-trained models with existing node weights are imported, edited to align with data input and output parameters, and trained on the relevant dataset. This method of implementing Deep Learning often shows higher performance in both classification and regression problems, as the models are exposed to exponentially higher volumes of data and may be less susceptible to common challenges such as overfitting to training data. 

While evaluating the effectiveness of pre-built and trained Deep Learning models in fraud detection is beyond the scope of the current project, communicating the potential for alternative approaches to creating robust fraud detection systems is. In many cases, transfer learning can simply be accomplished by importing a pre-built model into the workspace, adjusting its input and output layers to align with data and class label formats, and training on available data. The ability to transfer knowledge from one model to another is widely used across computer vision and object detection tasks.

### Model Creation

For this project, we will create a simple Deep Learning model comprised of 4 distinct layers of fully connected nodes. These layers are detailed below:

* *Input Layer* - An input layer expecting a flattened array of 21 distinct features to be passed through it.
* *First Dense Layer* - A dense layer of 64 neurons, in which each neuron receives input from each neuron in the preceding layer. The activation function is specified as relu, or Rectified Linear Unit, a linear activation function that outputs its input if it is positive or zero if it is negative.
* *Second Dense Layer* - An identical dense layer to the previous layer, comprised of 64 neurons that receive input from all neurons in the preceding layer. The relu activation function is again used to determine neuron outputs. 
* *Output Layer* - A final dense layer producing a single output informing the class label prediction. This layer employs a sigmoid activation function, which is a non-linear activation function that produces a value between zero and one representing the probability that the class prediction is not zero.

The code for instantiating this model is included below.

```
import tensorflow as tf
from tensorflow import keras

dl_model =   keras.Sequential([keras.layers.Flatten(input_shape = (21,)), 
             keras.layers.Dense(64, activation = tf.nn.relu), 
	     keras.layers.Dense(64, activation = tf.nn.relu),
             keras.layers.Dense(1, activation = tf.nn.sigmoid)])
```

After creating the model, it will be necessary to compile it to finalize its preparation, using a few important hyperparameters. A brief overview of the hyperparameters passed to through the model's compile method is included below:

* *Adam Optimizer* - Adam is an optimization function used to determine the rate at which network weights are adjusted based on training data, adapting learning weights bassed on calculations using the gradient of the loss function.
* *Binary Crossentropy Loss Function* - Binary Crossentropy is a loss function used in binary classification tasks, calculating a loss score based on the distance between actual class labels and predicted class labels.
* *Accuracy Metric* - As was discussed in the previous section, accuracy is not a meaningful metric to consider in binary classification problems. We will leverage a unique Keras metric, binary accuracy, to assess the effectiveness of our models in this project.

The code for compiling the model is included below. Similar to the Stacking Classifier that was built, no hyperparameter tuning or cross validation was performed with the model, highlighting an immediate area for future projects to build on this approach. This decision was again primarily made on the basis of resource constraints, and likely impacted resulting performance in novel classification tasks. The model as is is intended for use as a comparison relative to the Stacking Classifier, and not as a final version. 

```
dl_model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['binary_accuracy']) 
```

### Model Evaluation

After fitting the model to the training data, in a process much lengthier than the Stacking Classifier, predictions were again generated based on test data features and compared to the actual class labels for each data instance. The confusion matrices for both models, fit to the SMOTE data and fit to the ROS data, are included in Figures 10 and 11 below. We will again be using these matrices and combined F1 scores to assess the performance of the models when exposed to testing data.

**Figure 10** - Deep Learning Model classification matrix - SMOTE data.

![image](https://user-images.githubusercontent.com/80338181/183319614-d4ad874d-453b-4078-ad5a-554d8e346ee4.png#gh-light-mode-only)
![image](https://user-images.githubusercontent.com/80338181/183319641-af499421-4d1c-4e25-b8e4-dfe6caafdde7.png#gh-dark-mode-only)

**Figure 11** - Deep Learning Model classification matrix - ROS data.

![image](https://user-images.githubusercontent.com/80338181/183319671-f2445aad-6190-4500-bf6b-79b59f236d01.png#gh-light-mode-only)
![image](https://user-images.githubusercontent.com/80338181/183319716-04f82806-411a-4dab-8281-391c0f569a4d.png#gh-dark-mode-only)

As was seen among the Stacking Classifiers, the Deep Learning model trained on the SMOTE data slightly outperformed the one trained on the ROS data, showing a combined F1 score of 0.59 compared to 0.54. Both of these scores are lower than that shown by the Stacking Classifier trained on SMOTE data. Examining the confusion matrics, it again appears that the SMOTE model was more precise in its class label predictions, mislabeling legitimate transactions at a significantly lower rate. The ROS model, however, produced the lowest number of fraudulent transactions mislabeled as legitimate among all models, a factor that may be relevant to specific business cases. Both models output a high number of legitimate transactions misclassified as fraudulent, again portraying a "wide net" approach to classification.

## **Conclusions & Final Thoughts**

This project served as a starting point for outlining potential performance discrepancies between Ensemble and Deep Learning Models in classifying instances of digital transaction fraud, including a full-cycle overview of data cleaning and preparation. In assessing the utility of the data preparation approaches and created models discussed in this project, it is important to reference the business context in which fraud detection systems are expected to perform. There are a multitude of use cases for fraud detection systems, ranging from suspicious activity flags to authoritative transaction-locking mechanisms. Investigating and acting upon identified instances of digital transaction fraud can generally be considered a very active process, involving a high degree of resource commitments to handle both the fraudulent transaction and the committing party. 

It may be the case, then, that a model designer is interested in a model that flags as many instances of fraud as possible without flagging legitimate transactions, to mitigate the time and resources committed to investigating “cold” leads. However, it may also be the case that a "wider net" approach is desirable for its ability to prompt users about potentially suspicious activity detected in their transactions, wherein a model with less precision could be tolerable as consumers have an active role in examining and dismissing legitimate transactions that are classified incorrectly. 

### Performance & Comparison

### Future Directions for Similar Projects

While successful in identifying a discrepancy in predictive performance amoung unique data transformation and modeling approaches, there are a number of clear ways the present project could be improved to produce more robust fraud detection models.

* *Feature Engineering* - 
* *Hyperparameter Tuning* - 
* *Transfer Learning* - 

## **Project References**

1) https://www.mckinsey.com/industries/financial-services/our-insights/banking-matters/us-digital-payments-achieving-the-next-phase-of-consumer-engagement#:~:text=Although%20penetration%20of%20digital%20payments,to%20reach%20the%20remaining%20group.
2) https://www.frbsf.org/cash/publications/fed-notes/2019/june/2019-findings-from-the-diary-of-consumer-payment-choice/#:~:text=Consumers%20used%20cash%20in%2026,percentage%20point%20increase%20from%202017
3) https://resources.sift.com/ebook/digital-trust-safety-index-fraud-economy/
4) https://shiftprocessing.com/credit-card-fraud-statistics/
