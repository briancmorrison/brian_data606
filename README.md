# **Machine Learning for Digital Fraud Detection**
### Comparing the Performance of Stacking Classifier & Deep Learning Models in Identifying Instances of Digital Transaction Fraud



Brian Morrison

DATA606 - Capstone Project in Data Science

Professor Jay Wang

The University of Maryland, Baltimore County

## **Contents**
* [Introduction & Background](#introduction--background)

* [Dataset Overview](#dataset-overview)

* [Source Data Splitting](#source-data-splitting)

* [EDA & Dataset Preparation](#eda--dataset-preparation)

* [Ensemble Classification Model](#ensemble-classification-model)

* [Deep Learning Model](#deep-learning-model)

* [Conclusions & Final Thoughts](#conclusions--final-thoughts)

* [References](#project-references)

## **Introduction & Background**

The advent of cashless methods for payment transactions, such as credit cards, digital wallets, and buy now pay later (BNPY) services has incomparably augmented the way in which consumers interact with product and service providers. In fact, market research has indicated that as many as 80% of consumers used some form of digital payment - defined as including "...browser-based and in-app online purchases, in-store checkout using a mobile phone and/or QR code, and person-to-person payments..." in 2020, with nearly 60% of those consumers reporting using two or more forms of digital payment<sup>1</sup>. Cash and check transactions have been becoming increasingly rare relative to more convenient and secure payment methods, such as credit and debit cards - which accounted for over 50% of payment usage in 2019<sup>2</sup> and are almost ubiquitously linked to online transactional systems. While this shift has introduced a number of positive benefits to consumers, another area it has significantly affected is fraud; instances of attempted digital payment fraud have increased dramatically in the past few years, both in quantity and magnitude<sup>3,4</sup>. Given the compounding growth of digital transaction methods, and the resulting surge in fraudulent digital payment attempts, developing robust methods for identifying and classifying instances of fraud is a similarly compounding topic of interest to digital system providers.

Problem Statement: There are more ways to commit payment fraud today than there ever have been, and this statement will again be true in 10 years. Building and deploying a plethora of machine-learning based fraud detection systems, each with different approaches to classifying potential instances of fraud, is likely the best way to combat the wide range of ways individuals can commit fraud. In this project, I propose methods for constructing robust fraud detection models, with a specific emphasis on comparing the performance of Ensemble and Deep Learning based classification models.

## **Dataset Overview**

The dataset for this project is a large log of e-commerce transactions with a range of associated details. The dataset can be accessed [here](https://www.kaggle.com/competitions/ieee-fraud-detection/data). The data was made available through a Kaggle competition held by the IEEE Computational Intelligence Society (IEEE-CIS), and contains real-world ecommerce transactions provided by Vesta, an organization actively targeting digital fraud. 

The dataset is 1.35 GB in size, comprised of four csv files - two training and two testing files, separated by the type of features they contain. Two of the files contain transactional data, basic information about the transaction such as product and payment details, and two of the files contain identity data, information about the purchaser such as device and personal information. The transactional data contains 392 unique numerical and categorical features, while the identity data contains 40 unique numerical and categorical features. The transactions can be correlated between datasets by an identifying feature, 'TransactionID'. 

For this project, only the training dataset will be used, as it contains the target variable that models will be attempting to predict - 'isFraud', a binary variable indicating whether or not the transaction was fraudulent. The testing dataset's class labels were withheld by competition hosts, as test data predictions generated by models were submitted for consideration in project scoring. The training dataset contains over 590,000 individual transactions, with the proportion of fraudulent to legitimate transactions standing at nearly 1 to 30; roughly 3.5% of the dataset.

## **Source Data Splitting**

The first step in beginning this project is to migrate the dataset from its source location, Kaggle, to the project's GitHub repository. While GitHub provides a simple interface to manually upload data, there is a maximum file size limit of 50 MB. The maximum file size limit for data uploaded to a repository through the command line, however, is 100 MB - a much more reasonable limit considering our transactional dataset's size of over 600 MB.

In this notebook, we take steps to split the transactional dataset into 7 files below 100 MB to upload to the project's repository. By using pandas to import the data into a dataframe, we can easily split the dataset into multiple smaller dataframes ready for export. Importantly, these dataframe were split row-wise to support later concatenation through pandas .concat method. The identification dataset was itself below 100 MB, so it did not need to be split before being uploaded to the repository. 

Finally, a brief overview of the Git commands leveraged in uploading the data files to the project repository is provided for context. While many of these commands can be executed through a masked interface in the Visual Studio Code IDE, consideration of the Git commands popularly used for version control allows for some contextualization of the approach to data migration. An overview of the Git commands discussed is included below.

* `$git --version` - *Checking Git version*
* `$git clone https://github.com/briancmorrison/brian_data606.git` - *Cloning the project repository to make local edits*
* `$cd brian_data606` - *Navigating to cloned directory*
* `$git add ['filenames']` - *Adding files to local repository*
* `$git commit -m "Uploaded Source Data"` - *Commiting changes locally, with '-m' specifying the commit message*
* `$git push origin master` - *Pushing changes to GitHub repository, reconciling versions*

After confirming that the dataset additions were made to the GitHub repository, we are ready to move on to dataset exploration, cleaning, and preparation.

## **EDA & Dataset Preparation**

After handling necessary preparation steps for the source data files, the next step in the project is to explore, clean, and prepare data for introduction to machine learning models. Throughout the EDA & Dataset Preparation notebook, changes are made to align data parameters with those expected by many machine learning models - more specifically, changes are largely oriented towards gradient or distance-based models that may be sensitive to differences in the magnitude of variances in features. For example, a distance-based classifier may interpret differences in transaction amounts as more meaningful than differences in number of transactions made by cards due to large discrepancies in purchase amounts, despite the assumption in training that all features should be considered equally.

Thus, the desired outcome from this notebook is a dataset that contains only integer columns with no null values, and whose column ranges are scaled to be approximately (0, 1). To start, the split datasets were read into the notebook from the project's GitHub repository, and stored in a dictionary of pandas dataframes. The transactional dataframes were then concatenated using pandas .concat method. A left outer merge, using pandas .merge method, was then performed to combine the transaction and identity dataframes, with the column 'TransactionID' used as the merge key. A left outer merge was used to ensure all transactions were retained in the final dataset.

#### Null Handling & Feature Selection

After some basic data exploration, the first, and arguably most significant, decision to be made in this notebook is how to handle the high proportion of null values present in the dataset. Figure 1 below shows the proportion of null values in each column of the source dataset. The figure clearly displays an extremely high quantity of null values across a majority of features in the dataset, with only a small number of columns containing no null values. 

**Figure 1** - Proportion of null values in each of the combined dataset's 434 columns.

![image](https://user-images.githubusercontent.com/80338181/183307425-a5f42ec1-d7c7-434e-84fa-e007cd2faead.png#gh-light-mode-only)
![image](https://user-images.githubusercontent.com/80338181/183307470-2335ae61-f8c8-4b69-8568-0d1a4da2753d.png#gh-dark-mode-only)

There are a multitude of approaches to handling null values in a dataset, including value imputation, feature combination, dimensionality reduction, or simple removal. While each method generally has unique advantages and potential disadvantages, this project excluded any columns containing null values based on Google Colaboratory resource constraints. This is discussed in more detail in the future directions portion of the Conclusions & Final Thoughts section, dropping data points that may contain salient information for models to become attuned to is almost certain to degrade overall classification performance. 

This approach resulted in 20 remaining columns, with 1 being the Transaction ID identifying key and 1 being the isFraud class identifier - so a total of 18 features were retained for the models. They included the transaction date, transaction amount, product type, card type, and several counting features. One additional feature, card type, was also removed based on resource constraints, as encoding the categorical feature would have resulted in the addition of over 700 new columns. After this, 17 features remained for the models to be trained on.

#### Feature Encoding & Scaling

Following the removal of null values, the next step in preparing our dataset for introduction to machine learning models is encoding categorical variables. A significant number of categorical features contained null values and were therefore removed in the previous step. However, the remaining categorical column, product type, needed to be converted to a numerically typed column through One Hot Encoding - a process by which unique categories from the feature are "popped" out into separate columns, with binary values indicating whether or not the record is associated with the category. The process is relatively straightforward, and resulted in the addition of 5 new columns to the dataset.

After encoding our categorical feature, it was possible to move on to scaling data - a requirement for many distance and gradient based machine learning models. Feature scaling involves standardizing features to a range of approximately (0, 1) and a mean value of approximately 0. This allows equal consideration of features by distance or gradient based models, which might otherwise assume that a larger magnitude of variance in one feature relative to others indicates heightened importance of that feature. Scaling is also a prerequisite for Principal Components Analysis, a dimensionality reduction method that is discussed in more detail in the Minority Class Augmentation section. Boxplots will be used to visualize the range and approximate percentile distributions of our features to guide scaling, with Figure 2 belowing showing the distribution of features, excluding transaction datetime, prior to scaling.

**Figure 2** - Feature distributions prior to scaling.

![image](https://user-images.githubusercontent.com/80338181/183308390-fad2419a-08a9-4292-b6d3-ed37beb908c2.png#gh-light-mode-only)
![image](https://user-images.githubusercontent.com/80338181/183308507-ec7cca50-2eaf-453f-94bd-ce68fa1bb897.png#gh-dark-mode-only)

In selecting a suitable scaler to use on data, considering distributions and percentiles, as well as the presence of outliers, can help guide decision-making. For our primary features visualized above, 3 types of scalers were tested to assess their effectiveness. They are:

* *Standard Scaler* - A relatively simple scaling method where the mean of the distribution is subtracted from each value, and then values are scaled to unit variance - or divided by the standard deviation of the distribution.
* *Robust Scaler* - A more sophisticated scaling method that is intended to provide enhanced tolerance for extreme outliers in leptokurtic data. The median is removed from the distribution and data is scaled according to the Interquartile Range (IQR). 
* *MinMax Scaler* - A scaler designed for feature confined to a definite range with roughly uniform distribution. Values are scaled such that the minimum feature value is equal to 0, and the maximum feature value is equal to 1. 

The MinMax Scaler was shown to outperform the Standard Scaler across all columns, as was expected based on the non-Gaussian distributions of the features. A relatively unexpected outcome was the MinMax Scaler also outperforming the Robust Scaler across all columns, despite the high prevalence of outliers in some features. With some exceptions, outliers did not appear to significantly influence percentile ranges, which may explain why the simple MinMax Scaler performed so well relative to the other methods. Figure 3 below shows the previously displayed feature distributions following scaling. 

**Figure 3** - Feature distributions following scaling. 

![image](https://user-images.githubusercontent.com/80338181/183308415-3b1d8cd2-13f4-40b6-beac-5a9384416bc9.png#gh-light-mode-only)
![image](https://user-images.githubusercontent.com/80338181/183308542-eb619dd6-7306-4392-be83-837be452ebd9.png#gh-dark-mode-only)

#### Minority Class Augmentation

After scaling our features, the final step in preparing data to be passed to machine learning models is remedying the class imbalance present in the dataset. In many binary classification problems like transactional fraud detection, there are magnitudes fewer instances of one class than another. Considering the volume of transactions made through digital mediums every day, and the limited number of individuals attempting to commit fraud, it is no surprise that our dataset contains many more legitimate transactions than fraudulent ones. Figure 4 below displays the distribution of class labels across our dataset - instances labeled as fraud account for only 3.5% of the total volume of transactions.

**Figure 4** - The training data class imbalance visualized. 

![image](https://user-images.githubusercontent.com/80338181/183318868-3dcd96f7-94b4-47b1-8c49-18a37f11e0f7.png#gh-light-mode-only)
![image](https://user-images.githubusercontent.com/80338181/183318939-145ad114-26e1-4226-b9b4-3de3034945b4.png#gh-dark-mode-only)

In order to ensure machine learning models have a sufficient volume of data to be trained on with regards to the class of interest, it is important to artificially balance the quantities of data instances in each class. This intervention is called minority class augmentation, and can be done in a number of ways. Two common methods for augmenting minority classes are adjusting data sampling, by selectively oversampling the minority class or undersampling the majority class, and imputing novel data points, often through synthetic data generation techniques such as nearest neighbor algorithms. For this project, two approaches to incorporating additional data points into the minority class were selected, and are detailed below. 

* *SMOTE* - Synthetic Minority Over-Sampling Technique, or SMOTE, is a sophisticated approach to minority class augmentation that creates synthetic data points similar to existing data points. SMOTE employs a nearest neighbors approach to generating synthetic data points, by which neighbors close in feature space to existing minority class observations are generated. While a more advanced class balancing technique, this approach relies on the assumption that minority class observations are close in feature space to one another.

* *Random Oversampling* - Random Oversampling, or ROS, is a simpler approach to minority class augmentation that involves randomly sampling minority class observations to duplicate in the dataset, artificially balancing class weights across the dataset. This approach to handling class imbalances is particularly effective when minority class observations are not always close in feature space, meaning that synthetic data points generated through nearest neighbor algorithms may not accurately capture the salient features of minority class observations.

As is mentioned in the SMOTE overview, closeness in feature space of fraud instances - essentially, how similar they are across all of the features considered by the machine learning models - may impact the effectiveness of our class balancing techniques. An approach involving the creation of synthetic data points that are near neighbors to existing data points relies on the assumption that those instances are in fact near neighbors of one another. It is worth examining the distribution in a collapsed feature space of our data points to identify whether instances of fraud appear to show closeness to one another, using an approach called Principal Component Analysis (PCA).

##### PCA & Feature Space Evaluation

PCA is a method of dimensionality reduction often used to collapse a number of features into a smaller number of aggregate features that maintain similar directional trends. Briefly, PCA relies on computing the eigenvectors and eigenvalues of a covariance matrix generated by converting features into directional arrays, which are used to guide the construction of combined features that capture as much of the original variance in features as possible. Using PCA, we are able to collapse our feature space down to 3 aggregate features, and examine the distribution of fraud instances across these features. If fraud instances are close in fetaure space, it is likely an indication that SMOTE will be a more effective method for class balancing.

After fitting our PCA model to our data and ensuring that a significant amount of feature variance was captured by its componens (C1: 0.642, C2: 0.217 C3: 0.072), it is possible to visualize the results in three-dimensional space and assess the class distributions using hues. The outcome of this effort is shown in Figure 5 below, where the lighter blue points are instances of fraud.

**A Quick Note** - T-distributed stochastic neighbor embedding, or tSNE, is another popular approach to dimensionality reduction that is particulary well suited for visualizing high-dimensional data. 

**Figure 5** - Three-dimensional visualization of data distribution across principal components. 

![image](https://user-images.githubusercontent.com/80338181/183309155-d2167b57-a869-4c83-aeba-cec4697c0580.png#gh-light-mode-only)
![image](https://user-images.githubusercontent.com/80338181/183309202-f1f9f6b2-3115-4d0c-835d-9424e1366adb.png#gh-dark-mode-only)

It appears that the instances of fraud within the dataset are relatively close in feature space based on the principal components generated. For this reason, I would expect that SMOTE will perform well as a method for minority class augmentation. For the sake of comparison, we can generate separate training data for our SMOTE and ROS methods to compare resulting model performance, using a similar split and export approach as previously to make sure we can upload everything to the project's repository.

##### SMOTE

**Figure 6** - Visualization of the class distributions across the SMOTE dataset. 

![image](https://user-images.githubusercontent.com/80338181/183318997-7f964aee-6dc2-4c7d-81a9-088c642c1332.png#gh-light-mode-only)
![image](https://user-images.githubusercontent.com/80338181/183319123-eef68ba5-912f-4d63-8973-7170d807af76.png#gh-dark-mode-only)

##### Random Oversampling

**Figure 7** - Visualization of the class distributions across the ROS dataset.

![image](https://user-images.githubusercontent.com/80338181/183319161-be4bcbe4-429c-49ce-97f4-0668e8372650.png#gh-light-mode-only)
![image](https://user-images.githubusercontent.com/80338181/183319203-a0b53f1c-afe2-4dda-9496-717172e0701c.png#gh-dark-mode-only)

## **Ensemble Classification Model**

#### Lazy Predict

#### Model Creation

#### Model Evaluation

**Figure 8** - Ensemble Model classification matrix - SMOTE data.

![image](https://user-images.githubusercontent.com/80338181/183319363-8f89fae1-0803-493f-9498-860b9e67c54e.png#gh-light-mode-only)
![image](https://user-images.githubusercontent.com/80338181/183319514-67fd3652-186c-4f56-b736-bd344579515f.png#gh-dark-mode-only)

**Figure 9** - Ensemble Model classification matrix - ROS data.

![image](https://user-images.githubusercontent.com/80338181/183319470-ca4c339a-4d4f-4382-90bc-297ab0ab77ac.png#gh-light-mode-only)
![image](https://user-images.githubusercontent.com/80338181/183319555-11f13f7d-1019-4620-a6b7-95dcdebcea7d.png#gh-dark-mode-only)

## **Deep Learning Model**

#### Consideration - Transfer Learning

#### Model Creation

#### Model Evaluation

**Figure 10** - Deep Learning Model classification matrix - SMOTE data.

![image](https://user-images.githubusercontent.com/80338181/183319614-d4ad874d-453b-4078-ad5a-554d8e346ee4.png#gh-light-mode-only)
![image](https://user-images.githubusercontent.com/80338181/183319641-af499421-4d1c-4e25-b8e4-dfe6caafdde7.png#gh-dark-mode-only)

**Figure 11** - Deep Learning Model classification matrix - ROS data.

![image](https://user-images.githubusercontent.com/80338181/183319671-f2445aad-6190-4500-bf6b-79b59f236d01.png#gh-light-mode-only)
![image](https://user-images.githubusercontent.com/80338181/183319716-04f82806-411a-4dab-8281-391c0f569a4d.png#gh-dark-mode-only)

## **Conclusions & Final Thoughts**

#### Performance & Comparison

#### Future Directions for Similar Projects

* *Feature Engineering* - 
* *Hyperparameter Tuning* - 
* *Transfer Learning* - 

## **Project References**

1) https://www.mckinsey.com/industries/financial-services/our-insights/banking-matters/us-digital-payments-achieving-the-next-phase-of-consumer-engagement#:~:text=Although%20penetration%20of%20digital%20payments,to%20reach%20the%20remaining%20group.
2) https://www.frbsf.org/cash/publications/fed-notes/2019/june/2019-findings-from-the-diary-of-consumer-payment-choice/#:~:text=Consumers%20used%20cash%20in%2026,percentage%20point%20increase%20from%202017
3) https://resources.sift.com/ebook/digital-trust-safety-index-fraud-economy/
4) https://shiftprocessing.com/credit-card-fraud-statistics/
